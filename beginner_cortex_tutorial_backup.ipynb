{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tutorial-intro",
   "metadata": {},
   "source": [
    "# Learn Snowflake Cortex AI - Python Tutorial\n",
    "\n",
    "## Welcome to Your First AI-Powered Data Analysis!\n",
    "\n",
    "This tutorial will teach you how to use Python to talk to Snowflake's AI system called \"Cortex\". You'll learn to:\n",
    "- Ask questions in plain English\n",
    "- Have AI write SQL code for you\n",
    "- Get answers from your data\n",
    "\n",
    "**IMPORTANT**: Type every single line of code by hand. Do not copy and paste. This helps your brain learn the patterns.\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-explanation",
   "metadata": {},
   "source": [
    "## Step 1: Import the Tools We Need\n",
    "\n",
    "Before we can do anything, we need to tell Python which tools (called \"libraries\" or \"modules\") we want to use.\n",
    "\n",
    "Think of this like getting your toolbox ready before starting a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "source": [
    "# The 'import' keyword tells Python we want to use a tool\n",
    "# 'json' helps us work with data that looks like {\"key\": \"value\"}\n",
    "import json\n",
    "\n",
    "# This is Snowflake's special internal tool for talking to their AI\n",
    "# The underscore (_) at the start means it's an internal tool\n",
    "import _snowflake\n",
    "\n",
    "# This gets us connected to our Snowflake database\n",
    "# 'from X import Y' means \"from toolbox X, just get tool Y\"\n",
    "from snowflake.snowpark.context import get_active_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constants-explanation",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Our Configuration\n",
    "\n",
    "Now we need to set up some settings. These are like the \"address\" and \"phone number\" for Snowflake's AI system.\n",
    "\n",
    "In programming, we use ALL_CAPS names for settings that never change (called \"constants\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constants",
   "metadata": {},
   "source": [
    "# This is the \"web address\" where Snowflake's AI lives\n",
    "# Think of it like a URL, but for computer programs\n",
    "API_ENDPOINT = \"/api/v2/cortex/agent:run\"\n",
    "\n",
    "# This says \"wait up to 50 seconds for an answer\"\n",
    "# 50_000 milliseconds = 50 seconds (the underscores make big numbers easier to read)\n",
    "API_TIMEOUT_MS = 50_000\n",
    "\n",
    "# This tells Snowflake which AI brain to use - we're using Claude-4-Sonnet\n",
    "MODEL_NAME = \"claude-4-sonnet\"\n",
    "\n",
    "# This is the \"address\" of our search tool that looks through sales conversations\n",
    "CORTEX_SEARCH_SERVICES = \"sales_intelligence.data.sales_conversation_search\"\n",
    "\n",
    "# This is the \"address\" of our data model that knows about sales metrics\n",
    "SEMANTIC_MODELS = \"@sales_intelligence.data.models/sales_metrics_model_grc.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "session-explanation",
   "metadata": {},
   "source": [
    "## Step 3: Connect to Snowflake\n",
    "\n",
    "Now we need to connect to our Snowflake database. Think of this like logging into your email - we need to establish a connection before we can do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session",
   "metadata": {},
   "source": [
    "# This gets our current connection to Snowflake\n",
    "# 'session' is like a phone line - it stays open so we can keep talking to Snowflake\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-question-explanation",
   "metadata": {},
   "source": [
    "## Step 4: Ask Your First Question\n",
    "\n",
    "Now comes the fun part! We're going to ask the AI a question in plain English, and it will:\n",
    "1. Understand what we want\n",
    "2. Write SQL code to get the data\n",
    "3. Give us an answer\n",
    "\n",
    "Let's start with a simple question about sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-question",
   "metadata": {},
   "source": [
    "# This is our question in plain English - no programming knowledge needed!\n",
    "# We store it in a variable called 'prompt' (a prompt is a question for AI)\n",
    "prompt = \"Create a weekly sales metrics summary for the last 8 weeks. Include total revenue and win rate.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "payload-explanation",
   "metadata": {},
   "source": [
    "## Step 5: Package Our Request\n",
    "\n",
    "Now we need to package our question with instructions for the AI. This is like addressing an envelope - we need to tell it who should answer, what tools they can use, and what our question is.\n",
    "\n",
    "We use something called a \"dictionary\" (the curly braces `{}`) to organize this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "payload",
   "metadata": {},
   "source": [
    "# This is our \"package\" of information for the AI\n",
    "# The curly braces {} create a dictionary - think of it like a filing cabinet with labeled folders\n",
    "payload = {\n",
    "    # Tell the AI which \"brain\" to use\n",
    "    \"model\": MODEL_NAME,\n",
    "    \n",
    "    # This is our conversation with the AI\n",
    "    # Square brackets [] create a list - like a grocery list\n",
    "    \"messages\": [\n",
    "        {\n",
    "            # \"role\": \"user\" means this message is from us (the human)\n",
    "            \"role\": \"user\", \n",
    "            # \"content\" is what we're actually saying\n",
    "            \"content\": [\n",
    "                {\n",
    "                    # \"type\": \"text\" means we're sending words, not pictures\n",
    "                    \"type\": \"text\", \n",
    "                    # \"text\" is our actual question\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    # These are the \"tools\" the AI can use to answer our question\n",
    "    \"tools\": [\n",
    "        {\n",
    "            # This tool can write SQL code from our English question\n",
    "            \"tool_spec\": {\n",
    "                \"type\": \"cortex_analyst_text_to_sql\", \n",
    "                \"name\": \"analyst1\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            # This tool can search through conversations and documents\n",
    "            \"tool_spec\": {\n",
    "                \"type\": \"cortex_search\", \n",
    "                \"name\": \"search1\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    # These are the \"settings\" for each tool\n",
    "    \"tool_resources\": {\n",
    "        # Settings for the SQL-writing tool\n",
    "        \"analyst1\": {\n",
    "            # This tells the tool where to find information about our data structure\n",
    "            \"semantic_model_file\": SEMANTIC_MODELS\n",
    "        },\n",
    "        # Settings for the search tool\n",
    "        \"search1\": {\n",
    "            # Where to search\n",
    "            \"name\": CORTEX_SEARCH_SERVICES, \n",
    "            # Don't return more than 3 results\n",
    "            \"max_results\": 3, \n",
    "            # Use this column as the unique identifier\n",
    "            \"id_column\": \"conversation_id\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-call-explanation",
   "metadata": {},
   "source": [
    "## Step 6: Send Our Request to the AI\n",
    "\n",
    "Now we actually send our question to Snowflake's AI. This is like pressing \"Send\" on an email.\n",
    "\n",
    "We also need to check if something went wrong, like if the AI is busy or our internet is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-call",
   "metadata": {},
   "source": [
    "# Send our request to Snowflake's AI\n",
    "# This is like making a phone call - we dial the number (API_ENDPOINT) and talk (payload)\n",
    "resp = _snowflake.send_snow_api_request(\n",
    "    \"POST\",           # \"POST\" means we're sending data (like mailing a letter)\n",
    "    API_ENDPOINT,     # Where to send it\n",
    "    {},               # Headers (extra info) - empty for now\n",
    "    {},               # Parameters (settings) - empty for now  \n",
    "    payload,          # Our actual question and settings\n",
    "    None,             # Authentication (login info) - None means use current login\n",
    "    API_TIMEOUT_MS    # How long to wait for an answer\n",
    ")\n",
    "\n",
    "# Check if something went wrong\n",
    "# Status 200 means \"everything worked perfectly\"\n",
    "if resp.get(\"status\") != 200:\n",
    "    # If something went wrong, stop and show an error message\n",
    "    # The 'f' before the quotes lets us put variables inside the text\n",
    "    raise RuntimeError(f\"HTTP {resp.get('status')}: {resp.get('reason')} -> {resp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parsing-explanation",
   "metadata": {},
   "source": [
    "## Step 7: Understand the AI's Response\n",
    "\n",
    "The AI sends us back a lot of information, but it's in a special format. We need to extract the parts we care about:\n",
    "1. The AI's explanation in English\n",
    "2. The SQL code it wrote\n",
    "\n",
    "Think of this like opening a package and sorting out what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parsing",
   "metadata": {},
   "source": [
    "# The AI sends back JSON data (looks like {\"key\": \"value\"})\n",
    "# We convert this from text into something Python can understand\n",
    "events = json.loads(resp[\"content\"])\n",
    "\n",
    "# Create empty variables to store what we find\n",
    "# Think of these like empty boxes we'll fill up\n",
    "assistant_text = \"\"  # This will hold the AI's explanation\n",
    "generated_sql = \"\"   # This will hold the SQL code the AI wrote\n",
    "\n",
    "# Look through all the pieces of the AI's response\n",
    "# 'for' means \"do this for each item in the list\"\n",
    "for ev in events:\n",
    "    # We only care about \"message.delta\" events (these contain the actual answer)\n",
    "    # '.get()' safely gets a value - if it doesn't exist, it returns None instead of crashing\n",
    "    if ev.get(\"event\") != \"message.delta\":\n",
    "        continue  # Skip this event and go to the next one\n",
    "    \n",
    "    # Look through the content of this event\n",
    "    # This is like opening nested boxes inside boxes\n",
    "    for item in ev.get(\"data\", {}).get(\"delta\", {}).get(\"content\", []):\n",
    "        # If this piece is text (the AI's explanation)\n",
    "        if item.get(\"type\") == \"text\":\n",
    "            # Add it to our explanation box\n",
    "            # The '+=' means \"add this to what we already have\"\n",
    "            assistant_text += item.get(\"text\", \"\")\n",
    "        \n",
    "        # If this piece is tool results (SQL code and data)\n",
    "        elif item.get(\"type\") == \"tool_results\":\n",
    "            # Look through each result from the tools\n",
    "            for r in item.get(\"tool_results\", {}).get(\"content\", []):\n",
    "                # If this result is JSON data\n",
    "                if r.get(\"type\") == \"json\":\n",
    "                    # Get the JSON content\n",
    "                    j = r.get(\"json\", {})\n",
    "                    # Add any text explanation to our explanation box\n",
    "                    assistant_text += j.get(\"text\", \"\")\n",
    "                    # If there's SQL code, save it\n",
    "                    if j.get(\"sql\"):\n",
    "                        generated_sql = j[\"sql\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-explanation",
   "metadata": {},
   "source": [
    "## Step 8: Clean Up the Text\n",
    "\n",
    "Sometimes the AI includes special symbols that don't look nice. We'll clean those up to make the text more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "source": [
    "# Replace weird symbols with normal brackets\n",
    "# The AI sometimes uses special Unicode symbols that look strange\n",
    "# '.replace()' finds text and changes it to something else\n",
    "assistant_text = assistant_text.replace(\"„Äê‚Ä†\", \"[\").replace(\"‚Ä†„Äë\", \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "display-explanation",
   "metadata": {},
   "source": [
    "## Step 9: Show the Results\n",
    "\n",
    "Now let's see what the AI found for us! We'll print out:\n",
    "1. The AI's explanation in English\n",
    "2. The SQL code it wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-results",
   "metadata": {},
   "source": [
    "# Print the AI's explanation\n",
    "# '\\n' means \"start a new line\" - like pressing Enter\n",
    "print(\"\\n--- Assistant Text ---\\n\", assistant_text or \"(none)\")\n",
    "\n",
    "# Print the SQL code the AI wrote\n",
    "print(\"\\n--- Generated SQL ---\\n\", generated_sql or \"(none)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-sql-explanation",
   "metadata": {},
   "source": [
    "## Step 10: Run the SQL and See Our Data\n",
    "\n",
    "If the AI wrote SQL code for us, let's run it and see what data we get back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-sql",
   "metadata": {},
   "source": [
    "# Only run the SQL if we actually got some\n",
    "# 'if' means \"only do this when the condition is true\"\n",
    "if generated_sql:\n",
    "    # Clean up the SQL by removing semicolons (;) that might cause problems\n",
    "    # Then run it and convert the results to a pandas DataFrame (like a spreadsheet)\n",
    "    df = session.sql(generated_sql.replace(\";\", \"\")).to_pandas()\n",
    "    \n",
    "    # Show the first few rows of our results\n",
    "    # '.head()' means \"show me the beginning\"\n",
    "    print(\"\\n--- Query Results (first few rows) ---\\n\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-example-explanation",
   "metadata": {},
   "source": [
    "## Step 11: Try Another Question\n",
    "\n",
    "Great job! Now let's try a different question to see how the AI handles various types of requests.\n",
    "\n",
    "This time we'll ask about revenue by week - a common business question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-question",
   "metadata": {},
   "source": [
    "# Let's ask a different question\n",
    "prompt = \"Total revenue for all Closed Won Deals by week\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-payload-explanation",
   "metadata": {},
   "source": [
    "## Step 12: Package the Second Request\n",
    "\n",
    "We'll create the same type of package as before, but with our new question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-payload",
   "metadata": {},
   "source": [
    "# Create the same type of package, but with our new question\n",
    "payload = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"tool_spec\": {\n",
    "                \"type\": \"cortex_analyst_text_to_sql\", \n",
    "                \"name\": \"analyst1\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"tool_spec\": {\n",
    "                \"type\": \"cortex_search\", \n",
    "                \"name\": \"search1\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"tool_resources\": {\n",
    "        \"analyst1\": {\n",
    "            \"semantic_model_file\": SEMANTIC_MODELS\n",
    "        },\n",
    "        \"search1\": {\n",
    "            \"name\": CORTEX_SEARCH_SERVICES, \n",
    "            \"max_results\": 3, \n",
    "            \"id_column\": \"conversation_id\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-complete-explanation",
   "metadata": {},
   "source": [
    "## Step 13: Complete the Second Request\n",
    "\n",
    "Now we'll send our second question and process the response - same steps as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-complete",
   "metadata": {},
   "source": [
    "# Send the second request\n",
    "resp = _snowflake.send_snow_api_request(\n",
    "    \"POST\", \n",
    "    API_ENDPOINT, \n",
    "    {}, \n",
    "    {}, \n",
    "    payload, \n",
    "    None, \n",
    "    API_TIMEOUT_MS\n",
    ")\n",
    "\n",
    "# Check for errors\n",
    "if resp.get(\"status\") != 200:\n",
    "    raise RuntimeError(f\"HTTP {resp.get('status')}: {resp.get('reason')} -> {resp}\")\n",
    "\n",
    "# Parse the response\n",
    "events = json.loads(resp[\"content\"])\n",
    "\n",
    "# Reset our variables for the new response\n",
    "assistant_text = \"\"\n",
    "generated_sql = \"\"\n",
    "\n",
    "# Process the events (same logic as before)\n",
    "for ev in events:\n",
    "    if ev.get(\"event\") != \"message.delta\":\n",
    "        continue\n",
    "    for item in ev.get(\"data\", {}).get(\"delta\", {}).get(\"content\", []):\n",
    "        if item.get(\"type\") == \"text\":\n",
    "            assistant_text += item.get(\"text\", \"\")\n",
    "        elif item.get(\"type\") == \"tool_results\":\n",
    "            for r in item.get(\"tool_results\", {}).get(\"content\", []):\n",
    "                if r.get(\"type\") == \"json\":\n",
    "                    j = r.get(\"json\", {})\n",
    "                    assistant_text += j.get(\"text\", \"\")\n",
    "                    if j.get(\"sql\"):\n",
    "                        generated_sql = j[\"sql\"]\n",
    "\n",
    "# Clean up the text\n",
    "assistant_text = assistant_text.replace(\"„Äê‚Ä†\", \"[\").replace(\"‚Ä†„Äë\", \"]\")\n",
    "\n",
    "# Show the results\n",
    "print(\"\\n--- Assistant Text ---\\n\", assistant_text or \"(none)\")\n",
    "print(\"\\n--- Generated SQL ---\\n\", generated_sql or \"(none)\")\n",
    "\n",
    "# Run the SQL if we got some\n",
    "if generated_sql:\n",
    "    df = session.sql(generated_sql.replace(\";\", \"\")).to_pandas()\n",
    "    print(\"\\n--- Query Results (first few rows) ---\\n\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": "## Final Congratulations! üéâüéâ\n\nYou've now mastered an advanced AI workflow! You can:\n\n1. **Import Python libraries** and set up configurations\n2. **Ask questions in plain English** for both metrics and search\n3. **Use the Search tool** to find relevant conversations and documents\n4. **Use the SQL tool** to query structured business data\n5. **Chain tools together** to create powerful analysis workflows\n6. **Extract and use IDs** to connect different types of data\n7. **Handle complex JSON responses** with nested data structures\n8. **Run generated SQL** to get actionable business insights\n\n### Real-World Applications\n\nNow you can answer complex business questions like:\n- \"Which deals mentioned pricing concerns, and what are their values?\"\n- \"Find conversations about competitor mentions, then show me those deal details\"\n- \"Search for frustrated customers, then analyze their purchase history\"\n- \"Find deals mentioning specific features, then calculate their total pipeline value\"\n\n### Practice Ideas\n\nTry these two-step workflows:\n1. Search for conversations mentioning \"timeline\" ‚Üí Get deal close dates for those deals\n2. Search for \"decision maker\" mentions ‚Üí Show deal stages and amounts for those opportunities\n3. Search for product name mentions ‚Üí Calculate revenue by product category\n\n**Remember**: Keep typing everything by hand - this is how you'll truly master these patterns and become comfortable with AI-powered data analysis!"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 19: Understanding the Two-Tool Workflow\n\nCongratulations! You just learned how to use **both** Cortex tools together in a powerful workflow:\n\n### What Just Happened?\n\n1. **Search Tool First**: We asked \"Which deals have customers listed ROI or budget concerns?\" \n   - The AI recognized this needed to search through conversation text\n   - It used the **Cortex Search** tool to find relevant conversations\n   - We got back conversation IDs and text snippets\n\n2. **SQL Tool Second**: We asked for deal details using those conversation IDs\n   - The AI recognized this needed structured data from tables\n   - It used the **Cortex Analyst** tool to write SQL\n   - We got back actual deal records with amounts, stages, dates, etc.\n\n### Why This Is Powerful\n\n- **Search** finds the \"what\" and \"where\" - which conversations mentioned specific topics\n- **SQL** finds the \"who\" and \"how much\" - the actual business data behind those conversations\n- Together they let you go from \"I heard someone mention budget issues\" to \"Here are the $2.5M worth of deals that have budget concerns\"\n\n### Key Learning Points\n\n1. **Tool Selection**: The AI automatically chooses which tool to use based on your question\n2. **Chaining Results**: You can use results from one tool to inform questions for another tool\n3. **Business Value**: This workflow helps you connect unstructured conversations to structured business data\n4. **ID Linking**: Conversation IDs are the \"bridge\" that connects search results to database records",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Send the follow-up request\nfollowup_resp = _snowflake.send_snow_api_request(\n    \"POST\", \n    API_ENDPOINT, \n    {}, \n    {}, \n    followup_payload, \n    None, \n    API_TIMEOUT_MS\n)\n\n# Check for errors\nif followup_resp.get(\"status\") != 200:\n    raise RuntimeError(f\"HTTP {followup_resp.get('status')}: {followup_resp.get('reason')} -> {followup_resp}\")\n\n# Parse the follow-up response\nfollowup_events = json.loads(followup_resp[\"content\"])\n\n# Reset our variables for the follow-up response\nfollowup_assistant_text = \"\"\nfollowup_generated_sql = \"\"\n\n# Process the follow-up events (same logic as before)\nfor ev in followup_events:\n    if ev.get(\"event\") != \"message.delta\":\n        continue\n    for item in ev.get(\"data\", {}).get(\"delta\", {}).get(\"content\", []):\n        if item.get(\"type\") == \"text\":\n            followup_assistant_text += item.get(\"text\", \"\")\n        elif item.get(\"type\") == \"tool_results\":\n            for r in item.get(\"tool_results\", {}).get(\"content\", []):\n                if r.get(\"type\") == \"json\":\n                    j = r.get(\"json\", {})\n                    followup_assistant_text += j.get(\"text\", \"\")\n                    if j.get(\"sql\"):\n                        followup_generated_sql = j[\"sql\"]\n\n# Clean up the text\nfollowup_assistant_text = followup_assistant_text.replace(\"„Äê‚Ä†\", \"[\").replace(\"‚Ä†„Äë\", \"]\")\n\n# Show the follow-up results\nprint(\"\\n=== DEAL RECORDS QUERY ===\")\nprint(\"\\n--- AI Explanation ---\\n\", followup_assistant_text or \"(none)\")\nprint(\"\\n--- Generated SQL ---\\n\", followup_generated_sql or \"(none)\")\n\n# Run the SQL if we got some\nif followup_generated_sql:\n    followup_df = session.sql(followup_generated_sql.replace(\";\", \"\")).to_pandas()\n    print(\"\\n--- Deal Records with ROI/Budget Concerns ---\\n\")\n    print(followup_df.to_string())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create our follow-up request package\nfollowup_payload = {\n    \"model\": MODEL_NAME,\n    \"messages\": [\n        {\n            \"role\": \"user\", \n            \"content\": [\n                {\n                    \"type\": \"text\", \n                    \"text\": followup_prompt\n                }\n            ]\n        }\n    ],\n    \"tools\": [\n        {\n            \"tool_spec\": {\n                \"type\": \"cortex_analyst_text_to_sql\", \n                \"name\": \"analyst1\"\n            }\n        },\n        {\n            \"tool_spec\": {\n                \"type\": \"cortex_search\", \n                \"name\": \"search1\"\n            }\n        }\n    ],\n    \"tool_resources\": {\n        \"analyst1\": {\n            \"semantic_model_file\": SEMANTIC_MODELS\n        },\n        \"search1\": {\n            \"name\": CORTEX_SEARCH_SERVICES, \n            \"max_results\": 5, \n            \"id_column\": \"conversation_id\"\n        }\n    }\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 18: Query the Deals Table\n\nNow we'll send our follow-up question to get the actual deal records. The AI will use the SQL tool this time because we're asking for structured data from tables.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Now we'll ask for the actual deal records using those conversation IDs\n# This question will use the SQL tool instead of the search tool\nif conversation_ids:\n    # Create a question that asks for deal details using the conversation IDs\n    # We convert the list of IDs into a string that SQL can understand\n    id_list = \", \".join([f\"'{id}'\" for id in conversation_ids])\n    followup_prompt = f\"Show me all deal details for conversations with IDs: {id_list}. Include deal amount, stage, close date, and customer information.\"\nelse:\n    # If we didn't find any IDs, ask a general question\n    followup_prompt = \"Show me recent deals that mentioned budget or ROI concerns in their conversations.\"\n\nprint(f\"\\nFollow-up question: {followup_prompt}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extract conversation IDs from our search results\n# We'll use these to query the actual deals table\nconversation_ids = []\nif search_results:\n    # Go through each search result and get the conversation ID\n    for result in search_results:\n        conv_id = result.get('conversation_id')\n        if conv_id:\n            conversation_ids.append(conv_id)\n\nprint(f\"\\nFound {len(conversation_ids)} conversation IDs: {conversation_ids}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 17: Get the Actual Deal Records\n\nNow that we've found conversations mentioning ROI or budget concerns, let's get the actual deal records for those conversations!\n\nWe'll create a new question that asks for the deal details using the conversation IDs we just found.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Send the search request\nsearch_resp = _snowflake.send_snow_api_request(\n    \"POST\", \n    API_ENDPOINT, \n    {}, \n    {}, \n    search_payload, \n    None, \n    API_TIMEOUT_MS\n)\n\n# Check for errors\nif search_resp.get(\"status\") != 200:\n    raise RuntimeError(f\"HTTP {search_resp.get('status')}: {search_resp.get('reason')} -> {search_resp}\")\n\n# Parse the search response\nsearch_events = json.loads(search_resp[\"content\"])\n\n# Reset our variables for the search response\nsearch_assistant_text = \"\"\nsearch_generated_sql = \"\"\nsearch_results = []  # This will hold our search results\n\n# Process the search events\nfor ev in search_events:\n    if ev.get(\"event\") != \"message.delta\":\n        continue\n    for item in ev.get(\"data\", {}).get(\"delta\", {}).get(\"content\", []):\n        if item.get(\"type\") == \"text\":\n            search_assistant_text += item.get(\"text\", \"\")\n        elif item.get(\"type\") == \"tool_results\":\n            for r in item.get(\"tool_results\", {}).get(\"content\", []):\n                if r.get(\"type\") == \"json\":\n                    j = r.get(\"json\", {})\n                    search_assistant_text += j.get(\"text\", \"\")\n                    # Look for search results instead of SQL\n                    if j.get(\"results\"):\n                        search_results = j[\"results\"]\n                    # But also check if any SQL was generated\n                    if j.get(\"sql\"):\n                        search_generated_sql = j[\"sql\"]\n\n# Clean up the text\nsearch_assistant_text = search_assistant_text.replace(\"„Äê‚Ä†\", \"[\").replace(\"‚Ä†„Äë\", \"]\")\n\n# Show the search results\nprint(\"\\n=== SEARCH RESULTS ===\")\nprint(\"\\n--- AI Explanation ---\\n\", search_assistant_text or \"(none)\")\n\n# If we got search results, show them\nif search_results:\n    print(f\"\\n--- Found {len(search_results)} Conversations ---\")\n    for i, result in enumerate(search_results, 1):\n        print(f\"\\nConversation {i}:\")\n        # Show the conversation ID (we'll use this later)\n        print(f\"  ID: {result.get('conversation_id', 'Unknown')}\")\n        # Show a snippet of the conversation\n        print(f\"  Content: {result.get('chunk', 'No content')[:200]}...\")\n\n# If we got SQL instead, show that too\nif search_generated_sql:\n    print(\"\\n--- Generated SQL ---\\n\", search_generated_sql)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 16: Execute the Search Request\n\nLet's send our search request and see what conversations mention ROI or budget concerns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create our search request package\nsearch_payload = {\n    \"model\": MODEL_NAME,\n    \"messages\": [\n        {\n            \"role\": \"user\", \n            \"content\": [\n                {\n                    \"type\": \"text\", \n                    \"text\": search_prompt\n                }\n            ]\n        }\n    ],\n    # Same tools as before - the AI will choose which one to use\n    \"tools\": [\n        {\n            \"tool_spec\": {\n                \"type\": \"cortex_analyst_text_to_sql\", \n                \"name\": \"analyst1\"\n            }\n        },\n        {\n            \"tool_spec\": {\n                \"type\": \"cortex_search\", \n                \"name\": \"search1\"\n            }\n        }\n    ],\n    \"tool_resources\": {\n        \"analyst1\": {\n            \"semantic_model_file\": SEMANTIC_MODELS\n        },\n        \"search1\": {\n            \"name\": CORTEX_SEARCH_SERVICES, \n            # Let's get more results since we're searching\n            \"max_results\": 5, \n            \"id_column\": \"conversation_id\"\n        }\n    }\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 15: Package the Search Request\n\nThis request looks similar to before, but notice we're asking about conversation content. The AI will automatically choose the right tool (search vs. SQL) based on our question.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# This question will use the SEARCH tool to look through conversations\n# Notice how this is asking about conversation content, not data metrics\nsearch_prompt = \"Which deals have customers listed ROI or budget concerns?\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 14: Using Search to Find Specific Information\n\nNow let's learn something really powerful! Sometimes we want to search through conversations and documents to find specific information, then get the actual data records.\n\nWe'll use a two-step process:\n1. **First**: Search through conversations to find mentions of ROI or budget concerns\n2. **Second**: Use the conversation IDs we found to query the actual data table\n\nThis is like asking \"Which conversations mentioned budget issues?\" and then asking \"Show me the details of those specific deals.\"",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}